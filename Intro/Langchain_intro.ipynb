{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Environment Setup"
      ],
      "metadata": {
        "id": "jMSj7hqMxbJ6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWPoYgZ-xL1y",
        "outputId": "c8d14c2a-ea66-40f8-b4d3-8ccc61245597"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -q langchain langchain-nvidia-ai-endpoints gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"YOUR_API_KEY\"] = \"YOUR_API_KEY\"\n"
      ],
      "metadata": {
        "id": "jxUEk9d1xOJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
        "from functools import partial\n",
        "\n",
        "identity = RunnableLambda(lambda x: x)\n",
        "\n",
        "# Given an arbitrary function, you can make a runnable with it\n",
        "def print_and_return(x, preface=\"\"):\n",
        "    print(f\"{preface}{x}\")\n",
        "    return x\n",
        "\n",
        "# runnable lambda\n",
        "rprint0 = RunnableLambda(print_and_return)\n",
        "rprint1 = RunnableLambda(partial(print_and_return, preface=\"1: \"))\n",
        "\n",
        "## And you can use the same idea to make your own custom Runnable generator\n",
        "def RPrint(preface=\"\"):\n",
        "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
        "\n",
        "    ## Chaining two runnables\n",
        "chain1 = identity | rprint0\n",
        "chain1.invoke(\"Hello World!\")\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwdUuDaExQWm",
        "outputId": "62e278e1-143e-498b-d988-9d4ecf0db592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello World!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Chaining that one in as well\n",
        "output = (\n",
        "    chain1           ## Prints \"Welcome Home!\" & passes \"Welcome Home!\" onward\n",
        "    | rprint1        ## Prints \"1: Welcome Home!\" & passes \"Welcome Home!\" onward\n",
        "    | RPrint(\"2: \")  ## Prints \"2: Welcome Home!\" & passes \"Welcome Home!\" onward\n",
        ").invoke(\"Welcome Home!\")\n",
        "\n",
        "## Final Output Is Preserved As \"Welcome Home!\"\n",
        "print(\"\\nOutput:\", output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngBGWCJRxwoU",
        "outputId": "1d28da4d-0332-4a3d-f190-3fc3e92c1abb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome Home!\n",
            "1: Welcome Home!\n",
            "2: Welcome Home!\n",
            "\n",
            "Output: Welcome Home!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Simple LLM Chain"
      ],
      "metadata": {
        "id": "OuSuuZsuxy7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "## Simple Chat Pipeline\n",
        "chat_llm = ChatNVIDIA(model=\"meta/llama3-8b-instruct\")\n",
        "\n",
        "# Prompt\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Only respond in rhymes\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])\n",
        "\n",
        "# Chain\n",
        "rhyme_chain = prompt | chat_llm | StrOutputParser()\n",
        "\n",
        "print(rhyme_chain.invoke({\"input\" : \"I am best in world\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDCoZOP2yFNz",
        "outputId": "6351df34-4d72-46db-d7d0-1d2f52639ded"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A claim so bold and unfurled,\n",
            "You think you're top, without a whirl,\n",
            "A superstar, shining bright and bold,\n",
            "A hero of the world, your tale to be told!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def rhyme_chat(message, history):\n",
        "     return rhyme_chain.invoke({\"input\" : message})\n",
        "\n",
        "gr.ChatInterface(rhyme_chat).launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "Bjl1P6TRyFKZ",
        "outputId": "077fe1c7-67de-4b50-ec67-ce18a7be9737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py:334: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://b1837907a2e2d91347.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b1837907a2e2d91347.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Streaming Interface\n",
        "\n",
        "def rhyq(message, history):\n",
        "  buffer = ''\n",
        "  for token in rhyme_chain.stream({'input' : message}):\n",
        "    buffer += token\n",
        "    yield buffer\n",
        "\n",
        "demo = gr.ChatInterface(rhyq).queue()\n",
        "window_kwargs = {} # or {\"server_name\": \"0.0.0.0\", \"root_path\": \"/7860/\"}\n",
        "demo.launch(share=True, debug=True, **window_kwargs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "id": "wmYPqc_3yFHu",
        "outputId": "1e72febe-f2a0-45e7-ca40-75248f415d1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py:334: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://8833710f4f203e19bb.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8833710f4f203e19bb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://b1837907a2e2d91347.gradio.live\n",
            "Killing tunnel 127.0.0.1:7861 <> https://8833710f4f203e19bb.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Internal Response"
      ],
      "metadata": {
        "id": "__AFSzUoyFFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "instruct_llm = ChatNVIDIA(model=\"mistralai/mistral-7b-instruct-v0.2\")\n",
        "\n",
        "sys_msg = (\n",
        "    \"Choose the most likely topic classification given the sentence as context.\"\n",
        "    \" Only one word, no explanation.\\n[Options : {options}]\"\n",
        ")"
      ],
      "metadata": {
        "id": "alV3lPnA0cAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## One-shot classification prompt with heavy format assumptions.\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", sys_msg),\n",
        "    (\"user\", \"[[The sea is awesome]]\"),\n",
        "    (\"assistant\", \"boat\"),\n",
        "    (\"user\", \"[[{input}]]\"),\n",
        "])"
      ],
      "metadata": {
        "id": "XffmJ5Y00b98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zsc_prompt = ChatPromptTemplate.from_template(\n",
        "    f\"{sys_msg}\\n\\n\"\n",
        "    \"[[The sea is awesome]][/INST]boat</s><s>[INST]\"\n",
        "    \"[[{input}]]\"\n",
        ")"
      ],
      "metadata": {
        "id": "lIjYUh6F0b7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zsc_chain = zsc_prompt | instruct_llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "r1bfxblG1qEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def zsc_call(input, options=[\"car\", \"boat\", \"airplane\", \"bike\"]):\n",
        "    return zsc_chain.invoke({\"input\" : input, \"options\" : options}).split()[0]\n",
        "\n",
        "print(\"-\" * 80)\n",
        "print(zsc_call(\"Should I take the next exit, or keep going to the next one?\"))\n",
        "\n",
        "print(\"-\" * 80)\n",
        "print(zsc_call(\"I get seasick, so I think I'll pass on the trip\"))\n",
        "\n",
        "print(\"-\" * 80)\n",
        "print(zsc_call(\"I'm scared of heights, so flying probably isn't for me\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJuCZImh1ss5",
        "outputId": "68ea3965-e62d-4b4d-b13a-ee890d5aef35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "car\n",
            "--------------------------------------------------------------------------------\n",
            "boat\n",
            "--------------------------------------------------------------------------------\n",
            "airplane\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-component chain"
      ],
      "metadata": {
        "id": "I8lewxAe11T_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
        "from functools import partial\n",
        "\n",
        "def make_dictionary(v, key):\n",
        "    if isinstance(v, dict):\n",
        "        return v\n",
        "    return {key : v}\n",
        "\n",
        "def RInput(key='input'):\n",
        "    '''Coercing method to mold a value (i.e. string) to in-like dict'''\n",
        "    return RunnableLambda(partial(make_dictionary, key=key))\n",
        "\n",
        "def ROutput(key='output'):\n",
        "    '''Coercing method to mold a value (i.e. string) to out-like dict'''\n",
        "    return RunnableLambda(partial(make_dictionary, key=key))\n",
        "\n",
        "def RPrint(preface=\"\"):\n",
        "    return RunnableLambda(partial(print_and_return, preface=preface))"
      ],
      "metadata": {
        "id": "DT1eaCRG2PTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "up_and_down = (\n",
        "    RPrint(\"A: \")\n",
        "    ## Custom ensure-dictionary process\n",
        "    | RInput()\n",
        "    | RPrint(\"B: \")\n",
        "    ## Pull-values-from-dictionary utility\n",
        "    | itemgetter(\"input\")\n",
        "    | RPrint(\"C: \")\n",
        "    ## Anything-in Dictionary-out implicit map\n",
        "    | {\n",
        "        'word1' : (lambda x : x.split()[0]),\n",
        "        'word2' : (lambda x : x.split()[1]),\n",
        "        'words' : (lambda x: x),  ## <- == to RunnablePassthrough()\n",
        "    }\n",
        "    | RPrint(\"D: \")\n",
        "    | itemgetter(\"word1\")\n",
        "    | RPrint(\"E: \")\n",
        "    ## Anything-in anything-out lambda application\n",
        "    | RunnableLambda(lambda x: x.upper())\n",
        "    | RPrint(\"F: \")\n",
        "    ## Custom ensure-dictionary process\n",
        "    | ROutput()\n",
        ")\n",
        "\n",
        "up_and_down.invoke({\"input\" : \"Hello World\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1WrYpvH2PP1",
        "outputId": "a7f8d81b-e0b6-4209-c333-efd711052144"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A: {'input': 'Hello World'}\n",
            "B: {'input': 'Hello World'}\n",
            "C: Hello World\n",
            "D: {'word1': 'Hello', 'word2': 'World', 'words': 'Hello World'}\n",
            "E: Hello\n",
            "F: HELLO\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'output': 'HELLO'}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "up_and_down.invoke(\"Hello World\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wK83_mzt2PNY",
        "outputId": "e49e30a0-06bf-4354-82c9-304c6fe2d38f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A: Hello World\n",
            "B: {'input': 'Hello World'}\n",
            "C: Hello World\n",
            "D: {'word1': 'Hello', 'word2': 'World', 'words': 'Hello World'}\n",
            "E: Hello\n",
            "F: HELLO\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'output': 'HELLO'}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-w-5UQir2PLP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
##  Task 2: Build an Intelligent Question-Answering Chatbot with Memory and Search

Welcome to Task 2 of the LangChain Mastery Course!  
In this project, you'll build a **smart, memory-aware chatbot** that can **search through documents** and answer questions in a conversational, context-aware way.

---

###  Goal:
Create a **smart, conversational Q&A chatbot** that:
1. Can answer factual or context-specific questions using an LLM/chat model.
2. Searches a document or knowledge base using **embeddings and similarity search**.
3. Maintains **context over multiple turns** (i.e., remembers previous messages).


### ðŸ›  What Youâ€™ll Need to Use:
- An **LLM** or **Chat Model** like `ChatOpenAI`, `ChatAnthropic`, or a local Hugging Face model.
- An **Embedding Model** (e.g., OpenAI Embeddings or Hugging Face Sentence Transformers).
- A **Vector Store** (like FAISS or Chroma) to store and search embedded documents.
- Basic **memory integration** (e.g., `ConversationBufferMemory`) for multi-turn dialogue.


---
###  Minimum Requirements:
- A Streamlit or CLI interface for interaction.
- A text file (or PDF) that gets loaded and chunked using `TextLoader` or similar.
- Embedding and indexing of the document using `embed_documents`.
- User input gets embedded and matched against the vector store using similarity.
- Final answers are generated by the LLM using both user input and retrieved context.
- Memory is preserved across interactions.

---

##  Project Structure

| File/Folder                      | Description |
|----------------------------------|-------------|
| `main_chatbot.py`                | Main script to run the QA chatbot |
| `load_docs.py`                   | Load and split documents (PDFs, text, etc.) |
| `generate_embeddings.py`         | Generate and store document embeddings |
| `qa_retriever.py`                | Setup vector store and retrieval pipeline |
| `chat_with_memory.py`            | Integrate LLM with memory for conversation context |
| `streamlit_chat_ui.py`           | Streamlit app for a web-based chatbot UI |
| `docs/`                          | Folder for sample documents you want to search from |

---

##  Step-by-Step Instructions

### 1.  Prepare Your Documents
Place the files you want to query (e.g., `.txt`, `.pdf`) in the `docs/` folder.

> You can use the provided sample documents or add your own.

---

### 2.  Load and Split Documents
Use `load_docs.py` to load and chunk documents:

```bash
python load_docs.py
```
This script reads files from docs/, splits them into manageable chunks using LangChainâ€™s TextSplitter, and prepares them for embedding.

### 3.  Generate Embeddings
Run generate_embeddings.py to convert text chunks into vector embeddings and store them in a vector database (e.g., FAISS):

```bash
python generate_embeddings.py
```
You can choose between OpenAI or Hugging Face embedding models based on your preference or API availability.

### 4.  Set Up Retrieval Pipeline
The qa_retriever.py file defines how LangChain retrieves relevant chunks using semantic similarity when a question is asked.

You donâ€™t need to run this separately â€” it's used as a module in the chatbot.

### 5.  Add Conversation Memory
Use chat_with_memory.py to integrate an LLM with memory. This allows the bot to remember previous user inputs and provide more contextual answers.

Memory options include:
- ConversationBufferMemory â€“ stores full conversation history.
- ConversationSummaryMemory â€“ summarizes and stores only the essence.
  
### 6.  Run the Chatbot (CLI or Streamlit)
ðŸ”¸ CLI Version
Run the basic chatbot via terminal:
```bash
python main_chatbot.py
```
ðŸ”¸ Web UI (Streamlit)
Run the Streamlit interface:
```bash
streamlit run streamlit_chat_ui.py
```
This gives you a clean chat window in your browser. The backend will:
- Accept user input
- Search your document index for related chunks
- Use the LLM to answer using retrieved content
- Maintain context over multiple turns

### ðŸ§¾ Sample Prompt Flow:

```text
User: Who wrote the Federalist Papers?
Model: The Federalist Papers were written by Alexander Hamilton, James Madison, and John Jay.
User: What year was Hamilton born?
Model: Alexander Hamilton was born in 1755 or 1757, with historical debate about the exact year.
```
###ðŸ“¤ Submit Your Project:
Push your code to GitHub.

Include a README.md that explains how to run it, what models you used, and any optional features.

Share your repo link with the community or instructor for feedback!


>  Tip: This task simulates real-world use cases like PDF Q&A bots, AI tutors, and internal knowledge assistants.

---

## âœ… Final Result
By the end of this task, youâ€™ll have a conversational AI assistant capable of:
- Understanding follow-up questions
- Retrieving knowledge from your own documents
- Responding intelligently with context

> This is a foundational architecture for AI customer support, AI tutors, document Q&A bots, and much more!

Happy building! ðŸ’¡

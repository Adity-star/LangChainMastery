{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Document Loader\n",
        "Document Loader is a class that loads Documents from various sources.\n",
        "\n",
        "Listed below are some examples of Document Loaders.\n",
        "\n",
        "- PyPDFLoader: Loads PDF files\n",
        "- CSVLoader: Loads CSV files\n",
        "- UnstructuredHTMLLoader: Loads HTML files\n",
        "- JSONLoader: Loads JSON files\n",
        "- TextLoader: Loads text files\n",
        "- DirectoryLoader: Loads documents from a directory"
      ],
      "metadata": {
        "id": "MBbA2k_-DFZ6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UldSIwZUCSuT"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain_community pypdf arxiv pymupdf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. PyPDF\n",
        "PyPDF is a widely-used Python library for reading and extracting text from PDF files. LangChain integrates with PyPDF through PyPDFLoader, allowing you to easily convert PDF documents into structured Document objects that include both the content and metadata (like page numbers)."
      ],
      "metadata": {
        "id": "iaXDpE1kGBgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader('/content/MIASDB Excerpta Medica 1994.pdf')\n",
        "\n",
        "# Load data into Document objects\n",
        "docs = loader.load()\n",
        "\n",
        "# Print the contents of the document\n",
        "print(docs[0].page_content[:300])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhB2zs1uF3_d",
        "outputId": "05ced375-55e1-4130-aa18-ec09aad7a072"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "See\tdiscussions,\tstats,\tand\tauthor\tprofiles\tfor\tthis\tpublication\tat:\t\n",
            "http://www.researchgate.net/publication/243788073\n",
            "The\tmammographic\timage\tanalysis\tsociety\n",
            "digital\tmammogram\tdatabase.\tExerpta\tMedica\n",
            "ARTICLE\n",
            "\t·\tJANUARY\t1994\n",
            "CITATIONS\n",
            "170\n",
            "3\tAUTHORS\n",
            ",\tINCLUDING:\n",
            "John\tSuckling\n",
            "University\tof\tCambridg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. WebBaseLoader\n",
        "WebBaseLoader is a specialized document loader in LangChain designed for processing web-based content.\n",
        "\n",
        "It leverages the BeautifulSoup4 library to parse web pages effectively, offering customizable parsing options through SoupStrainer and additional bs4 parameters.\n",
        "\n",
        "This tutorial demonstrates how to use WebBaseLoader to:\n",
        "- Load and parse web documents effectively\n",
        "- Customize parsing behavior using BeautifulSoup options\n",
        "- Handle different web content structures flexibly."
      ],
      "metadata": {
        "id": "JGEgOfJWF362"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "# Load news article content using WebBaseLoader\n",
        "loader = WebBaseLoader(\n",
        "   web_paths=(\"https://techcrunch.com/2024/12/28/google-ceo-says-ai-model-gemini-will-the-companys-biggest-focus-in-2025/\",),\n",
        "   # Configure BeautifulSoup to parse only specific div elements\n",
        "   bs_kwargs=dict(\n",
        "       parse_only=bs4.SoupStrainer(\n",
        "           \"div\",\n",
        "           attrs={\"class\": [\"entry-content wp-block-post-content is-layout-constrained wp-block-post-content-is-layout-constrained\"]},\n",
        "       )\n",
        "   ),\n",
        "   # Set user agent in request header to mimic browser\n",
        "   header_template={\n",
        "       \"User_Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36\",\n",
        "   },\n",
        ")\n",
        "\n",
        "# Load and process the documents\n",
        "docs = loader.load()\n",
        "print(f\"Number of documents: {len(docs)}\")\n",
        "docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zyt_X3vTF334",
        "outputId": "fce34534-dc36-4249-d2c4-6bbae5e0f8c2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents: 1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'https://techcrunch.com/2024/12/28/google-ceo-says-ai-model-gemini-will-the-companys-biggest-focus-in-2025/'}, page_content='\\nGoogle CEO Sundar Pichai reportedly told Google employees that 2025 will be a “critical” year for the company.\\nCNBC reports that it obtained audio from a December 18 strategy meeting where Pichai and other executives put on ugly holiday sweaters and laid out their priorities for the coming year.\\n\\n\\n\\n\\n\\n\\n\\n\\n“I think 2025 will be critical,” Pichai said. “I think it’s really important we internalize the urgency of this moment, and need to move faster as a company. The stakes are high.”\\nThe moment, of course, is one where tech companies like Google are making heavy investments in AI, and often with mixed results. Pichai acknowledged that the company has some catching up to do on the AI side — he described the Gemini app (based on the company’s AI model of the same name) as having “strong momentum,” while also acknowledging “we have some work to do in 2025 to close the gap and establish a leadership position there as well.”\\n“Scaling Gemini on the consumer side will be our biggest focus next year,” he said.\\n\\nTechCrunch has an AI-focused newsletter! Sign up here to get it in your inbox every Wednesday.\\n\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bypass SSL certificate verification\n",
        "loader.requests_kwargs = {\"verify\": False}\n",
        "\n",
        "# Load documents from the web\n",
        "docs = loader.load()\n",
        "docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmDPI9fqF31j",
        "outputId": "a8d39b55-31ec-4276-c436-7341484b37c1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'techcrunch.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'https://techcrunch.com/2024/12/28/google-ceo-says-ai-model-gemini-will-the-companys-biggest-focus-in-2025/'}, page_content='\\nGoogle CEO Sundar Pichai reportedly told Google employees that 2025 will be a “critical” year for the company.\\nCNBC reports that it obtained audio from a December 18 strategy meeting where Pichai and other executives put on ugly holiday sweaters and laid out their priorities for the coming year.\\n\\n\\n\\n\\n\\n\\n\\n\\n“I think 2025 will be critical,” Pichai said. “I think it’s really important we internalize the urgency of this moment, and need to move faster as a company. The stakes are high.”\\nThe moment, of course, is one where tech companies like Google are making heavy investments in AI, and often with mixed results. Pichai acknowledged that the company has some catching up to do on the AI side — he described the Gemini app (based on the company’s AI model of the same name) as having “strong momentum,” while also acknowledging “we have some work to do in 2025 to close the gap and establish a leadership position there as well.”\\n“Scaling Gemini on the consumer side will be our biggest focus next year,” he said.\\n\\nTechCrunch has an AI-focused newsletter! Sign up here to get it in your inbox every Wednesday.\\n\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. CSV Loader\n",
        "LangChain's CSVLoader allows you to load structured CSV data into a list of Document objects, making it ready for use with language models in tasks like:\n",
        "\n",
        "- Summarization\n",
        "- Q&A over tabular data\n",
        "- Semantic search or vector storage"
      ],
      "metadata": {
        "id": "UpW04EuRF3y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import CSVLoader\n",
        "\n",
        "loader = CSVLoader(\"/content/avocado.csv\")\n",
        "\n",
        "docs = loader.load()\n",
        "\n",
        "for record in docs[:2]:\n",
        "  print(record)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ht9vs7fTF3wW",
        "outputId": "aa076cb2-bc97-4cda-bf81-49983c089cc8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content=': 0\n",
            "Date: 2015-12-27\n",
            "AveragePrice: 1.33\n",
            "Total Volume: 64236.62\n",
            "4046: 1036.74\n",
            "4225: 54454.85\n",
            "4770: 48.16\n",
            "Total Bags: 8696.87\n",
            "Small Bags: 8603.62\n",
            "Large Bags: 93.25\n",
            "XLarge Bags: 0.0\n",
            "type: conventional\n",
            "year: 2015\n",
            "region: Albany' metadata={'source': '/content/avocado.csv', 'row': 0}\n",
            "page_content=': 1\n",
            "Date: 2015-12-20\n",
            "AveragePrice: 1.35\n",
            "Total Volume: 54876.98\n",
            "4046: 674.28\n",
            "4225: 44638.81\n",
            "4770: 58.33\n",
            "Total Bags: 9505.56\n",
            "Small Bags: 9408.07\n",
            "Large Bags: 97.49\n",
            "XLarge Bags: 0.0\n",
            "type: conventional\n",
            "year: 2015\n",
            "region: Albany' metadata={'source': '/content/avocado.csv', 'row': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[1].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sDeMR0RIdg2",
        "outputId": "19050898-fe76-48c9-c5c0-112c25ae0d8a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ": 1\n",
            "Date: 2015-12-20\n",
            "AveragePrice: 1.35\n",
            "Total Volume: 54876.98\n",
            "4046: 674.28\n",
            "4225: 44638.81\n",
            "4770: 58.33\n",
            "Total Bags: 9505.56\n",
            "Small Bags: 9408.07\n",
            "Large Bags: 97.49\n",
            "XLarge Bags: 0.0\n",
            "type: conventional\n",
            "year: 2015\n",
            "region: Albany\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. ArvixLoader\n",
        "arXiv is an open access archive for 2 million scholarly articles in the fields of physics,\n",
        "\n",
        "mathematics, computer science, quantitative biology, quantitative finance, statistics, electrical engineering and systems\n",
        "\n",
        "science, and economics.\n",
        "To access the Arxiv document loader, you need to install arxiv, PyMuPDF and langchain-community integration packages.\n",
        "\n",
        "PyMuPDF converts PDF files downloaded from arxiv.org into text format."
      ],
      "metadata": {
        "id": "TXn8eGukIf3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import ArxivLoader\n",
        "\n",
        "### Enter the research topic you want to search for in the Query parameter\n",
        "loader = ArxivLoader(\n",
        "    query=\"Chain of thought\",\n",
        "    load_max_docs=2,  # max number of documents\n",
        "    load_all_available_meta=True,  # load all available metadata\n",
        ")"
      ],
      "metadata": {
        "id": "WiUv0oT1I_-a"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first document's content and metadata\n",
        "docs = loader.load()\n",
        "print(docs[0].page_content[:100])\n",
        "print(docs[0].metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouuo8GQCI_8P",
        "outputId": "c86f035c-f574-46a2-a6f5-d4dee373d7a0"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contrastive Chain-of-Thought Prompting\n",
            "Yew Ken Chia∗1,\n",
            "Guizhen Chen∗1, 2\n",
            "Luu Anh Tuan2\n",
            "Soujanya Pori\n",
            "{'Published': '2023-11-15', 'Title': 'Contrastive Chain-of-Thought Prompting', 'Authors': 'Yew Ken Chia, Guizhen Chen, Luu Anh Tuan, Soujanya Poria, Lidong Bing', 'Summary': 'Despite the success of chain of thought in enhancing language model\\nreasoning, the underlying process remains less well understood. Although\\nlogically sound reasoning appears inherently crucial for chain of thought,\\nprior studies surprisingly reveal minimal impact when using invalid\\ndemonstrations instead. Furthermore, the conventional chain of thought does not\\ninform language models on what mistakes to avoid, which potentially leads to\\nmore errors. Hence, inspired by how humans can learn from both positive and\\nnegative examples, we propose contrastive chain of thought to enhance language\\nmodel reasoning. Compared to the conventional chain of thought, our approach\\nprovides both valid and invalid reasoning demonstrations, to guide the model to\\nreason step-by-step while reducing reasoning mistakes. To improve\\ngeneralization, we introduce an automatic method to construct contrastive\\ndemonstrations. Our experiments on reasoning benchmarks demonstrate that\\ncontrastive chain of thought can serve as a general enhancement of\\nchain-of-thought prompting.', 'entry_id': 'http://arxiv.org/abs/2311.09277v1', 'published_first_time': '2023-11-15', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CL', 'categories': ['cs.CL'], 'links': ['http://arxiv.org/abs/2311.09277v1', 'http://arxiv.org/pdf/2311.09277v1']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. TextLoader\n",
        "This tutorial focuses on using LangChain’s TextLoader to efficiently load and process individual text files."
      ],
      "metadata": {
        "id": "y1Sw0-O1I_50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "# Create a text loader\n",
        "loader = TextLoader(\"/content/Text summarization.txt\", encoding=\"utf-8\")\n",
        "\n",
        "# Load the document\n",
        "docs = loader.load()\n",
        "print(f\"Number of documents: {len(docs)}\\n\")\n",
        "print(\"[Metadata]\\n\")\n",
        "print(docs[0].metadata)\n",
        "print(\"\\n========= [Preview - First 500 Characters] =========\\n\")\n",
        "print(docs[0].page_content[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPANkaZGI_3P",
        "outputId": "e2436541-8e57-4cbe-8f5a-48002fecf3b9"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents: 1\n",
            "\n",
            "[Metadata]\n",
            "\n",
            "{'source': '/content/Text summarization.txt'}\n",
            "\n",
            "========= [Preview - First 500 Characters] =========\n",
            "\n",
            "Project Title:\n",
            "Text Summarization Assistant\n",
            "\n",
            "Objective:\n",
            "The goal of this project is to develop an intelligent assistant capable of generating concise and informative summaries from longer bodies of text. The assistant will support extractive and abstractive summarization techniques and be designed to help project managers quickly digest reports, meeting transcripts, documentation, and other text-heavy resources.\n",
            "\n",
            "Key Features:\n",
            "\n",
            "Input Processing: Accepts plain text or documents (PDF, DOCX, etc.) \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Directory Loader"
      ],
      "metadata": {
        "id": "aU3ybtHZJvFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "\n",
        "path = \"/content/sample_data\"\n",
        "\n",
        "text_loader_kwargs = {\"autodetect_encoding\": True}\n",
        "\n",
        "loader = DirectoryLoader(\n",
        "    path,\n",
        "    loader_cls=TextLoader,\n",
        "    silent_errors=True,\n",
        "    loader_kwargs=text_loader_kwargs,\n",
        ")\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "XMq7m0quJ1r5"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_sources = [doc.metadata[\"source\"] for doc in docs]\n",
        "doc_sources"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jcf2sI7hJ1oU",
        "outputId": "749569fa-3dcd-4753-e8cb-db5cc5db5be9"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/sample_data/anscombe.json',\n",
              " '/content/sample_data/README.md',\n",
              " '/content/sample_data/california_housing_train.csv',\n",
              " '/content/sample_data/california_housing_test.csv',\n",
              " '/content/sample_data/mnist_test.csv',\n",
              " '/content/sample_data/mnist_train_small.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"[Metadata]\\n\")\n",
        "print(docs[0].metadata)\n",
        "print(\"\\n========= [Preview - First 500 Characters] =========\\n\")\n",
        "print(docs[0].page_content[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXPZpxelJ1mE",
        "outputId": "8cb116da-2fa0-4644-cca9-ab4aedcf2ab7"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Metadata]\n",
            "\n",
            "{'source': '/content/sample_data/anscombe.json'}\n",
            "\n",
            "========= [Preview - First 500 Characters] =========\n",
            "\n",
            "[\n",
            "  {\"Series\":\"I\", \"X\":10.0, \"Y\":8.04},\n",
            "  {\"Series\":\"I\", \"X\":8.0, \"Y\":6.95},\n",
            "  {\"Series\":\"I\", \"X\":13.0, \"Y\":7.58},\n",
            "  {\"Series\":\"I\", \"X\":9.0, \"Y\":8.81},\n",
            "  {\"Series\":\"I\", \"X\":11.0, \"Y\":8.33},\n",
            "  {\"Series\":\"I\", \"X\":14.0, \"Y\":9.96},\n",
            "  {\"Series\":\"I\", \"X\":6.0, \"Y\":7.24},\n",
            "  {\"Series\":\"I\", \"X\":4.0, \"Y\":4.26},\n",
            "  {\"Series\":\"I\", \"X\":12.0, \"Y\":10.84},\n",
            "  {\"Series\":\"I\", \"X\":7.0, \"Y\":4.81},\n",
            "  {\"Series\":\"I\", \"X\":5.0, \"Y\":5.68},\n",
            "\n",
            "  {\"Series\":\"II\", \"X\":10.0, \"Y\":9.14},\n",
            "  {\"Series\":\"II\", \"X\":8.0, \"Y\":8.14},\n",
            "  {\"Ser\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"[Metadata]\\n\")\n",
        "print(docs[1].metadata)\n",
        "print(\"\\n========= [Preview - First 500 Characters] =========\\n\")\n",
        "print(docs[1].page_content[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mL6ljtG-J1jf",
        "outputId": "4394aa0e-892c-4b6f-8412-c3c77abebe40"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Metadata]\n",
            "\n",
            "{'source': '/content/sample_data/README.md'}\n",
            "\n",
            "========= [Preview - First 500 Characters] =========\n",
            "\n",
            "This directory includes a few sample datasets to get you started.\n",
            "\n",
            "*   `california_housing_data*.csv` is California housing data from the 1990 US\n",
            "    Census; more information is available at:\n",
            "    https://docs.google.com/document/d/e/2PACX-1vRhYtsvc5eOR2FWNCwaBiKL6suIOrxJig8LcSBbmCbyYsayia_DvPOOBlXZ4CAlQ5nlDD8kTaIDRwrN/pub\n",
            "\n",
            "*   `mnist_*.csv` is a small sample of the\n",
            "    [MNIST database](https://en.wikipedia.org/wiki/MNIST_database), which is\n",
            "    described at: http://yann.lecun.com/exdb/mnist/\n",
            "\n",
            "* \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"[Metadata]\\n\")\n",
        "print(docs[3].metadata)\n",
        "print(\"\\n========= [Preview - First 500 Characters] =========\\n\")\n",
        "print(docs[3].page_content[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yv9F_PDaKJKv",
        "outputId": "bb585e6a-5cde-4d54-bba5-5c4a9d9e0751"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Metadata]\n",
            "\n",
            "{'source': '/content/sample_data/california_housing_test.csv'}\n",
            "\n",
            "========= [Preview - First 500 Characters] =========\n",
            "\n",
            "\"longitude\",\"latitude\",\"housing_median_age\",\"total_rooms\",\"total_bedrooms\",\"population\",\"households\",\"median_income\",\"median_house_value\"\n",
            "-122.050000,37.370000,27.000000,3885.000000,661.000000,1537.000000,606.000000,6.608500,344700.000000\n",
            "-118.300000,34.260000,43.000000,1510.000000,310.000000,809.000000,277.000000,3.599000,176500.000000\n",
            "-117.810000,33.780000,27.000000,3589.000000,507.000000,1484.000000,495.000000,5.793400,270500.000000\n",
            "-118.360000,33.820000,28.000000,67.000000,15.000000,49.00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2EMDmscYKKu0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
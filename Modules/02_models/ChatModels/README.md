# üí¨ Understanding Chat Models in LangChain
Chat models are designed specifically for **multi-turn, conversational interactions**. They‚Äôre ideal for applications like chatbots, virtual assistants, AI tutors, or anything requiring dynamic, context-aware dialogue.
This section of the course will help you understand how LangChain works with chat models across different providers, and how you can build intelligent, interactive agents with minimal code.

---

## How Chat Models Work with LLMs

Modern LLMs are typically accessed via **chat models** that take a list of messages as input and return a response message. This conversational interface enables **multi-turn dialogues** where the model can keep track of context and provide more relevant responses.

### Key Capabilities of New-Generation Chat Models

1. **Tool Calling**  
   Many chat models offer a **native tool calling API** that allows developers to build **rich applications**. This feature enables LLMs to interact with external services, APIs, and databases. Tool calling can also be used to extract **structured information** from unstructured data and perform various tasks.

2. **Structured Output**  
   Some chat models support generating **structured output**, such as JSON, that adheres to a predefined schema. This is particularly useful for applications that require consistent and predictable data formats.

3. **Multimodality**  
   Newer chat models are **multimodal**, meaning they can work with data types beyond text. For example, these models can process **images**, **audio**, and **video**, opening up possibilities for more complex and interactive applications.

---

## Features of LangChain for Working with Chat Models

LangChain provides a **consistent interface** for interacting with chat models from different providers while offering a range of powerful features to optimize application performance.

### Key Features

- **Integrations with Multiple Providers**  
  LangChain integrates with various popular chat model providers, including:
  - **Anthropic**
  - **OpenAI**
  - **Ollama**
  - **Microsoft Azure**
  - **Google Vertex**
  - **Amazon Bedrock**
  - **Hugging Face**
  - **Cohere**
  - **Groq**  
  For an up-to-date list of supported models, check the [Chat Model Integrations page](https://python.langchain.com/docs/integrations/chat/).

- **Support for Multiple Message Formats**  
  You can use either LangChain's own **message format** or **OpenAI's message format**, depending on the provider and your needs.

- **Standard Tool Calling API**  
  LangChain offers a **standard interface** for:
  - Binding tools to models
  - Accessing tool call requests made by models
  - Sending tool results back to the model

- **Structured Output**  
  Use the `with_structured_output` method to structure the model's output in a standardized format like JSON.

- **Async Programming & Efficient Batching**  
  LangChain supports **asynchronous programming** for handling multiple requests simultaneously, and provides tools for **efficient batching** of messages.

- **Rich Streaming API**  
  Stream real-time responses from your chat models with LangChain's **streaming API**.

- **LangSmith Integration**  
  LangChain integrates with **LangSmith** to help you monitor and debug your production-grade LLM applications.

- **Additional Features**  
  LangChain offers additional tools like:
  - **Standardized token usage**
  - **Rate limiting**
  - **Caching**  
  These features help optimize the performance and efficiency of your applications.

---

## üîß How LangChain Handles Chat Models

LangChain provides a high-level interface for working with chat models through its `ChatModel` classes like `ChatOpenAI`, `ChatAnthropic`, and others.
Instead of writing plain prompts, you build conversations using LangChain's message classes:

- `system` messages ‚Äì for setting behavior and context  
- `user` messages ‚Äì representing input from the user  
- `assistant` messages ‚Äì generated by the model in response

This structure allows chat models to better understand the context of a conversation and generate more coherent, relevant responses.

Example:
```python
from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage

chat = ChatOpenAI()
messages = [
    SystemMessage(content="You are a helpful assistant."),
    HumanMessage(content="What's the capital of France?")
]
response = chat(messages)
print(response.content)
```
## üîå Chat Model Integrations in LangChain

LangChain offers extensive support for **chat model integrations** from a variety of providers, allowing developers to easily switch between or combine models depending on their needs.

These integrations fall into two categories:

### ‚úÖ Official Models
- Officially supported by LangChain and/or the model provider.
- Available via specific `langchain-<provider>` packages.
- Examples include: `langchain-openai`, `langchain-anthropic`, `langchain-azure`, etc.

### üåç Community Models
- Contributed and supported by the community.
- Found in the `langchain-community` package.
- Great for experimenting with open-source or niche models.


## üß© Interface: `BaseChatModel`

All chat models in LangChain implement the `BaseChatModel` interface, which also extends the **Runnable** interface. This enables advanced features such as:

- **Streaming support**
- **Asynchronous execution**
- **Batch processing**
- **Tool integration**
- **Structured output generation**

### üéØ Standard Inputs & Outputs

Chat models operate on **lists of messages** as input and return one or more **messages** as output. Each message includes:

- A **role**: `"system"`, `"user"`, or `"assistant"`
- A **content block**: Text, and in some cases, multimodal content (e.g., images, audio)

### üì® Supported Message Formats

LangChain supports two message formats for interacting with chat models:

- **LangChain Message Format** (default): Internally used across LangChain APIs.
- **OpenAI Message Format**: Compatible with OpenAI‚Äôs chat API format.

---

## ‚öôÔ∏è Key Methods in Chat Models

| Method | Description |
|--------|-------------|
| `invoke(messages)` | Primary method for calling a chat model. Accepts a list of messages and returns a response. |
| `stream(messages)` | Streams the model's output token-by-token in real time. |
| `batch(inputs)` | Send multiple conversations in a single call for optimized performance. |
| `bind_tools(tools)` | Bind external tools to the model, allowing dynamic function calling. |
| `with_structured_output(schema)` | Enables structured outputs such as JSON by wrapping the invoke method with validation. |

---

## ‚öôÔ∏è Standard Parameters

LangChain chat models expose a common set of parameters that configure model behavior. Common parameters include:

- `temperature`: Controls randomness in output
- `max_tokens`: Limits the number of tokens in the response (not supported by all providers)
- `timeout`: Sets a time limit for the response

> ‚ö†Ô∏è **Note:**  
> These parameters only apply to models that support them. Some providers (especially in the community package) might not expose all standard options.

---

## üìò Naming Convention

LangChain follows a naming convention where all chat model classes are prefixed with **`Chat`**. Examples include:

- `ChatOpenAI`
- `ChatAnthropic`
- `ChatOllama`
- `ChatGoogleVertexAI`

This makes it easy to identify which classes are chat-capable.

---

## üìÅ Folder Structure & Examples

This folder contains implementations using different chat model providers:

| File                      | Description                                                       |
|---------------------------|-------------------------------------------------------------------|
| [`openai_chat_model.py`](https://github.com/Adity-star/LangChainMastery/blob/main/Modules/02_models/ChatModels/openai_chat_model.py)    | Using OpenAI‚Äôs ChatGPT models with LangChain                     |
| [`huggingface_chat_model.py`](https://github.com/Adity-star/LangChainMastery/blob/main/Modules/02_models/ChatModels/huggingface_chat_model.py) | Example using Hugging Face‚Äôs chat-compatible models             |
| [`local_llama_chat_model.py`](https://github.com/Adity-star/LangChainMastery/blob/main/Modules/02_models/ChatModels/local_llama_chat_model.py) | Running a local LLaMA-based chat model with LangChain          |
| [`anthropic_chat_model.py`](https://github.com/Adity-star/LangChainMastery/blob/main/Modules/02_models/ChatModels/anthropic_chat_model.py) | Integration with Anthropic's Claude via ChatAnthropic            |
| [`streamlit_chatbot.py`](https://github.com/Adity-star/LangChainMastery/blob/main/Modules/02_models/ChatModels/streamlit_chatbot.py)    | Streamlit-powered UI for an interactive chatbot                  |

---
### üåê  Providers Covered
[OpenAI (ChatGPT)](https://platform.openai.com/docs/overview)
- Widely used for general-purpose conversational AI.
- Supports message roles and advanced control.
- OpenAI API Docs
  
[Hugging Face](https://huggingface.co/)
- Access open-source chat models like Mistral, Falcon, or ChatGLM.
- Can run locally (via Transformers) or use Hugging Face Inference API.
- Great for customizing and experimenting with different architectures
  
[Local LLaMA](https://ollama.com/)
- Run models on your own hardware for privacy, speed, or offline use.
- Ideal for edge devices or controlled environments.
- Often used with quantized versions for better performance on CPUs.

[Anthropic (Claude)](https://claude.ai/login)
- Claude is designed for safer and more human-aligned conversations.
- LangChain integrates Claude using the `ChatAnthropic` wrapper.
- Great for sensitive applications or enterprise deployments.


---
### üíª Streamlit Chatbot Interface
The [streamlit_chatbot.py](https://github.com/Adity-star/LangChainMastery/blob/main/Modules/02_models/ChatModels/streamlit_chatbot.py)script provides a ready-to-run Streamlit app for chatting with your selected model.

To run it:
```bash
streamlit run streamlit_chatbot.py
```
You can modify the script to switch models, customize the system prompt, or connect it with tools like vector stores and memory.

### Want to Contribute?
Feel free to open issues, submit PRs, or suggest new providers and examples. Whether you're improving documentation or adding new integrations, contributions are always welcome!

---

### Next Steps 
Now that you‚Äôve learned how to work with **Chat Models**, it‚Äôs time to explore how **embedding models** are used to enable *search*, *retrieval*, *similarity*, and *context-aware reasoning*.

Ready for next step? Let's go [**Embedding Models**](https://github.com/Adity-star/LangChainMastery/tree/main/Modules/02_models/EmbeddingModels)
